{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## kNN: k-Nearest-Neighbor"
      ],
      "metadata": {
        "id": "DJZU99izo5id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest-Neighbor es un algoritmo SUPERVISADO de Machine Learning. Se usa para clasificar nuevas muestras (valores discretos) o para predecir (regresión, valores continuos). Al ser un método sencillo, es ideal para hacer pruebas y jugar con el. Sirve esencialmente para clasificar valores buscando los puntos de datos “más similares” (por cercanía) aprendidos en la etapa de entrenamiento y tratar de predecir puntos basados en esa clasificación."
      ],
      "metadata": {
        "id": "BtluioMBo4ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia de K-means, que es un algoritmo NO SUPERVISADO y donde la “K” significa la cantidad de “grupos” (clusters) que deseamos clasificar, en k-Nearest Neighbor la “k” significa la cantidad de “puntos vecinos” que tenemos en cuenta en las cercanías para clasificar los “n” grupos que ya se conocemos de antemano, pues es un algoritmo supervisado."
      ],
      "metadata": {
        "id": "24F27CJppXeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Qué es el algoritmo k-Nearest Neighbor?\n",
        "\n",
        "Es un método que simplemente busca en las observaciones más cercanas a la que se está tratando de predecir y clasifica el punto de interés basado en la mayoría de datos que le rodean.\n",
        "\n",
        "Como sabemos, es un algoritmo SUPERVISADO, que quiere decir que tenemos etiquetado nuestro conjunto de datos de entrenamiento, con la clase o resultado esperado dada “una fila” de datos. y ademas, esta basado en Instancia, que quiere decir que nuestro algoritmo no aprende explícitamente un modelo (como por ejemplo en Regresión Logística o árboles de decisión), sino que memoriza las instancias de entrenamiento que son usadas como “base de conocimiento” para la fase de predicción."
      ],
      "metadata": {
        "id": "sa_fdV9xpq4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aunque es bastante sencillo, se utiliza en la resolución de multitud de problemas, como en sistemas de recomendación, búsqueda semántica o detección de anomalías entre otros. Como contra, sabemos que utiliza todo el dataset para entrenar “cada punto” y por eso requiere de uso de mucha memoria y recursos de procesamiento (CPU). Por estas razones kNN tiende a funcionar mejor en datasets pequeños y sin una cantidad enorme de features (las columnas).   \n",
        "\n",
        "Si tenemos muchas features, podriamos usar un PCA para reducirlas y luego aplicar kNN."
      ],
      "metadata": {
        "id": "BhbW08cSqKj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ¿Cómo funciona kNN?\n",
        "\n",
        "Primero calculamos la distancia entre el item a clasificar y el resto de items del dataset de entrenamiento. Luego seleccionar los “k” elementos más cercanos (con menor distancia, según la función que se use). Una vez hecho esto, realizamos una “votación de mayoría” entre los k puntos: los de una clase/etiqueta que dominen decidirán su clasificación final. Sin embargo, esto tiene un problema, y es que, ¿qué pasa si hay dos (o más) clases con el mismo número de votos? En ese caso, no habría un valor predominante, por lo que este sistema no serviría. En estos casos, el algoritmo aumenta en 1 la k, es decir, añade un nuevo vecino que será el que (probablemente) desempate. Si añadiendo la k en 1 no se desempata, se vuelve incrementar en 1 hasta que se de ese desempate.\n",
        "\n",
        "Teniendo en cuenta el ultimo paso, veremos que para decidir la clase de un punto es muy importante el valor de k, pues este terminará casi por definir a qué grupo pertenecerán los puntos, sobre todo en las “fronteras” entre grupos. Por ejemplo, y a priori, lo logico seria elegir valores impares de k para desempatar (si las features que utilizamos son pares). No será lo mismo tomar para decidir 3 valores que 13. Esto no quiere decir que necesariamente tomar más puntos implique mejorar la precisión. Lo que es seguro es que cuantos más “puntos k”, más tardará nuestro algoritmo en procesar y darnos respuesta.\n",
        "\n",
        "Las formas más populares de “medir la cercanía” entre puntos son la distancia Euclidiana, que es la de siempre, o la Cosine Similarity, que mide el ángulo de  los vectores, cuanto menores, serán mas similares. Recordemos que este algoritmo y prácticamente todos en ML funcionan mejor con varias características de las que tomemos datos (las columnas de nuestro dataset). Lo que entendemos como “distancia” en la vida real, quedará abstracto a muchas dimensiones que no podemos “visualizar” fácilmente (como por ejemplo en un mapa).\n",
        "\n",
        "![1_9dIHpqBHgWocQXaQSQCFuw.gif](https://miro.medium.com/max/1400/1*9dIHpqBHgWocQXaQSQCFuw.gif)\n"
      ],
      "metadata": {
        "id": "Ds246p9gqzhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MBYflInPs5Uy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}